Meta-reviewer: Hello, Dialogue Agent. I am the meta-reviewer assigned to review the paper titled "Deepening Hidden Representations from Pre-trained Language Models". Can you tell me more about the reviews we received for this paper?

Dialogue Agent: Good day, Meta-reviewer. Certainly, we received three reviews for this paper. Review 1 appreciated the paper's well-written structure and experimental evaluation, but was concerned about the complexity of the proposed extension and the limited improvement in performance. Review 2 found the paper useful but criticized the limited novelty of the proposed components and the need for additional experiments on more challenging tasks. Review 3 suggested that the proposed approach lacked novelty and that the experiments were not well-designed, calling for more generalizability and better comparisons.

Meta-reviewer: Thank you for summarizing that for me. From what you said, it seems like the reviewers have differing opinions about the paper. Based on their feedback, what can we conclude about the paper's acceptance or rejection?

Dialogue Agent: That is correct, Meta-reviewer. The reviews vary in their assessment of the paper's quality and contribution. However, it is important to note that Review 1 was marginally above the acceptance threshold, while Review 2 was marginally below it, and Review 3 recommended rejection. As such, there is no clear consensus for the paper's acceptability. It will be up to you to weigh the reviews against the importance of reviewer confidence to decide on whether to accept or reject the paper.

Meta-reviewer: That's helpful to know. Can you tell me more about the proposed approach and why it was evaluated on the GLUE benchmark?

Dialogue Agent: Certainly. The paper proposes a technique called Hidden Representation Extractor (HIRE) that aims to improve the performance of pre-trained language models on downstream tasks. HIRE consists of two parts: the Hidden Representation Extractor itself and a Fusion network that combines the extracted features with the original features of the pre-trained model. HIRE works by dynamically deciding which intermediate layers to use depending on the input example and using a mechanism conceptually similar to self-attention to yield a complementary representation to the last layer, which is then used for prediction.

The paper was evaluated on the GLUE benchmark, as this is a widely used benchmark for natural language understanding tasks, which the proposed technique aims to improve. However, as Review 2 pointed out, it may be more beneficial to evaluate the technique on more challenging tasks like QA (e.g., SQuAD 1.1/2.0) to better demonstrate its effectiveness.

Meta-reviewer: That makes sense. Thank you for clarifying that. Lastly, can you tell me what suggestions the reviewers provided for the authors?

Dialogue Agent: Certainly. Review 1 suggested evaluating the technique on the SuperGLUE benchmark to better showcase its potential, trying different downstream tasks to increase the chance of significant improvements, and including an experiment that adds several self-attention layers on top of the pre-trained model to see if similar improvements can be achieved. Review 2 suggested evaluating the technique on more challenging tasks like QA and testing the technique under a multi-task setting. Review 3 recommended the authors to explore the generalizability of the technique by applying it to various pre-trained language models and languages and carrying out more experiments to demonstrate the novelty and effectiveness of the proposed components.

Meta-reviewer: Thank you for relaying that information. Based on the reviews and their suggestions, I believe the paper could benefit from some additional experiments and a more comprehensive analysis of its usefulness. However, given that the reviewers had varying opinions and that Review 1 was marginally above the acceptance threshold, I will decide to provisionally accept the paper, with the caveat that the authors address some of the reviewers' concerns as part of the revision process.