Meta-Reviewer: Hello dialogue agent, I'm here to discuss the reviews for the paper "Understanding Composition of Word Embeddings via Tensor Decomposition." Can you give me a brief summary of the paper's contributions and methods?

Dialogue Agent: Certainly. The paper proposes a method for creating combined low-dimensional representations for combinations of pairs of words that have a specific syntactic relationship, such as adjective-noun pairs. The method uses a core tensor from the Tucker decomposition of a three-way PMI tensor to generate an additive term that is used in the composition of two word embedding vectors. The authors show that their model outperforms weighted versions of additive compositionality in terms of spearman and pearson correlation with human judgments on adjective-noun pairs. The paper's contributions include proposing a tensor decomposition method for understanding composition of word embeddings and illustrating its effectiveness on specific word pairs.

Meta-Reviewer: Thank you. Reviewer 1 seems to have some concerns about the phrase similarity part of the paper. Can you tell me more about that?

Dialogue Agent: Reviewer 1 acknowledged the theoretical correctness of the model, but was not convinced by the empirical results on phrase similarity. The reviewer was unsure if adding the tensor term to the sum of two base embedding vectors really improves the performance of the model for phrase similarity. However, the reviewer still recommended accepting the paper.

Meta-Reviewer: I see. Reviewer 2 pointed out that the paper has potential, but has some limitations. Can you elaborate on that?

Dialogue Agent: Reviewer 2 states that the theoretical justification and the learned tensor model provided by the paper are interesting, but the main appeal of the paper is the potential for more performant word embeddings. However, the paper does not show that these embeddings are useful in downstream applications. The higher-order features T*a*b are useful only when a is a noun and b is an adjective. The reviewer wishes that the paper had investigated using T to model higher-order interactions for all (a,b) pairs. Additionally, the reviewer thinks that the paper should have compared its method to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper. Nevertheless, the reviewer considers the paper marginally above the acceptance threshold.

Meta-Reviewer: I see. Reviewer 3 seems to have more critical feedback about the paper. Can you highlight their main points?

Dialogue Agent: Reviewer 3 is critical of the paper's lack of supporting explanations for the assumptions it makes, particularly the lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by the proposed method makes this a good idea. The reviewer also pointed out that the paper does not provide enough grounding for its claims or decisions. For example, the paper incorrectly claims that the Arora's RAND-WALK model does not capture any syntactic information, when it actually does capture syntactic information to a lesser extent than other popular methods for word embedding. Additionally, the paper does not provide any justification for weighing the tensor term in the phrase similarity experiment, despite the fact that the unweighted version of the model performed worse than the additive composition. The reviewer believes that the paper marginally meets the acceptance threshold.

Meta-Reviewer: Thank you for summarizing the reviews. It seems like there are some concerns about the paper's empirical results and the lack of theoretical insights, but overall the reviewers believe that the paper meets the acceptance threshold. Considering the reviewer confidence levels, I think I will accept the paper with minor revisions to address the reviewers' concerns.