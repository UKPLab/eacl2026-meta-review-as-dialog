Meta-reviewer: Thank you for providing the reviews. Can you tell me more about the paper's main contributions and limitations?

Dialogue agent: The paper proposes a method for training deep neural networks using the information bottleneck principle on a layer-by-layer basis. The method estimates the mutual information between high-dimensional variables using a noise-regularized estimator, and connects the information bottleneck theory with weight decay. The paper claims to validate the information bottleneck theory and shows that layer-wise training can improve prediction accuracy. However, the reviews raise concerns about the method's claims and experiments, including the lack of comparative studies with other methods, the unclear scope of the paper, and issues with the mutual information estimator.

Meta-reviewer: I see. Review 1 mentions some interesting aspects of the paper while expressing some doubts about its contributions. Do you think their concerns are valid?

Dialogue agent: Review 1 acknowledges the paper's interesting ideas but questions the significance of the findings. The reviewer also notes the technical problems in defining mutual information due to the degenerate joint distribution between inputs and latent compression. However, the paper overcomes this problem by using a noisy version of the latent compression, which is an "ad hoc" fix. Review 1 also suggests that the results seem to contain some degree of over-selling. Overall, while the reviewer is marginally below the acceptance threshold, their concerns are valid, and their review offers constructive feedback.

Meta-reviewer: Review 2, on the other hand, strongly rejects the paper and mentions several technical and experimental problems. Do you think these issues are critical?

Dialogue agent: Review 2 expresses strong objections to the paper, highlighting many technical and experimental problems, including the lack of comparative studies with other methods, the unclear scope of the paper, issues with the mutual information estimator, and problems with the Discriminator used for the objective. The reviewer concludes that the paper falls flat and that its experiments are extremely lacking. While the confidence level is high, the reviewer's evaluation should be taken with caution, and the concerns raised should be addressed in the final decision.

Meta-reviewer: Review 3 provides some constructive feedback and expresses some reservations about the paper's conclusions. Do you think their comments are relevant?

Dialogue agent: Review 3 acknowledges the paper's careful methods for estimating mutual information and connecting the information bottleneck theory with weight decay. However, the reviewer also raises concerns about the paper's unclear scope, the lack of comparative studies, and issues with the beta selection in the experiments. They suggest that the paper only compares the layer-wise IB objective with the cross-entropy loss, rather than comparing the layer-wise training with end-to-end training while keeping the objective constant. The reviewer also questions the findings about the effectiveness of layer-wise training. Overall, the reviewer's feedback is relevant and should be considered in the final decision.