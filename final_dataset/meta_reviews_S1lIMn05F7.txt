Meta-reviewer: Hello, dialogue agent. Can you summarize the main contributions and novelty of the paper "A Direct Approach to Robust Deep Learning Using Adversarial Networks"?

Dialogue agent: Certainly, meta-reviewer. The paper proposes a new approach for dealing with adversarial instances in deep learning by using a GAN-based method. It introduces a way to dynamically generate adversarial examples on-the-fly using a generator, and these examples are consumed for training the discriminator to identify adversarial samples from clean ones. The novelty of this approach includes the use of a generator that produces adversarial noise for its input clean image, which is a departure from the traditional method of pre-training neural networks for generating adversarial examples.

Meta-reviewer: That sounds interesting. However, I noticed that Reviewer 1 mentioned two similar techniques proposed in the past by Xiao et al. (2018) and Lee et al. (2017). How does the proposed method differ from these previous works?

Dialogue agent: The main difference between the proposed method and the works of Xiao et al. (2018) and Lee et al. (2017) lies in the use of a generator that produces adversarial noise for its input clean image on-the-fly for training the discriminator. Additionally, the authors of the paper compare their proposed approach with a variety of strong defenses, including the ensemble adversarial training and PGD adversarial training, supporting their approach with robust experiments.

Meta-reviewer: That's helpful. However, Reviewer 1 also pointed out that the paper failed to compare against targeted attacks such as T-FGS, C&W, or GAN-attack. Can you provide any insight into why this was not done?

Dialogue agent: As per the authors' analysis, these attacks are computationally expensive and require additional models to be pre-trained against these attacks, making these comparisons difficult to execute. The authors believe that their approach addresses the critical issue of defending against both white and black box attacks, which is typically harder than targeting a specific adversary.

Meta-reviewer: That's understandable. Moving forward, Reviewer 2 noted some minor concerns about the paper's structure and presentation. Can you provide any further insights on these?

Dialogue agent: Reviewer 2 mentions a small issue with Equation 1 regarding the missing support for the argmax. The authors could clarify this in the next revision. Moreover, the reviewer pointed out that the usefulness of the generator is only in attacking its own model and questions if the attacks produced by the generator are transferable to other models. The authors could include a brief discussion on the transferability of the attacks generated by the generator. Additionally, Reviewer 2 notes that Tables 1 and 2 are described as giving attack accuracies, but the scores reported are classification accuracy. The authors could modify the descriptions of these tables to reflect defense accuracy instead of attack accuracy.

Meta-reviewer: Thank you for the clarification. Finally, Reviewer 3 highlights the simplicity of the approach in dealing with adversarial attacks, but also raises some concerns about its novelty and the need for more insights on why it works well. Can you provide your thoughts on these?

Dialogue agent: The authors admit that the network structure may not be novel, but it still performs well. They suggest that the proposed approach works because it is a simple yet effective application of GANs. The authors could provide more insights on why it works well in the next revision. Additionally, Reviewer 3 suggests that visualizing the behavior of their design and providing side-to-side comparisons with other approaches could be helpful for future revisions. 

Meta-reviewer: Thank you for the helpful summaries and insights, dialogue agent. Based on these reviews, I believe that the paper requires some revisions before acceptance. However, I am moderately confident in my evaluation.