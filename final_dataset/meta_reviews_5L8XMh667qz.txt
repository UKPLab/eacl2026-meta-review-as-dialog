Meta-Reviewer: Thank you for meeting with me today to discuss the paper "Encoded Prior Sliced Wasserstein AutoEncoder for learning latent manifold representations". As you know, we have received three reviews and I would like to discuss them with you before making my final decision on whether to accept or reject the paper.

Dialogue Agent: Of course, I am here to answer any questions you may have and provide any necessary clarification.

Meta-Reviewer: Great, let's start with Review 1. The reviewer recommends rejection of the paper based on their evaluation of the experiments. They suggest that the hypotheses are not clearly supported and that the visual results are not comparable with state-of-the-art methods on the CelebA dataset. What are your thoughts on this evaluation?

Dialogue Agent: While we respect the reviewer's opinion, we believe that our paper makes a significant contribution to the literature on representation learning with VAEs. We introduce an additional prior-encoder network to autoencoders to learn an unconstrained prior. We also apply the sliced Wasserstein distance for matching between the posterior and the prior, enhance the conventional SWD with non-linear transformations, and make the latent space similar to the feature space with a structural consistency loss. In addition, we propose a novel graph-based algorithm for minimizing the pathwise energy to achieve manifold walking to improve interpolation in the latent space. Although the reviewer suggests that we should have compared our results to more recent methods and used popular metrics like FID, we believe that our contributions are significant and our empirical results are promising.

Meta-Reviewer: Thank you for that explanation. Moving on to Review 2, the reviewer suggests that the motivation for using nonlinear transformation is not convincing and that the evaluation is insufficient. They state that the paper should compare with more recent methods and use popular metrics. What do you think of this evaluation?

Dialogue Agent: We appreciate the reviewer's input and we have taken their comments into consideration. As for the motivation for using nonlinear transformation, we believe that it improves the efficiency of the approximation of the Wasserstein distance, as discussed in related works. However, we understand that the justification for our specific choice of Equation 6 may not be entirely clear. We will make an effort to provide more explanation in the revised version of the paper. Furthermore, we recognize that using more recent methods and popular metrics may provide a more complete study and we will certainly consider this suggestion in future work.

Meta-Reviewer: That sounds like a reasonable approach. Finally, Review 3 notes that the paper's pipeline is clear and easy to understand, and that the usage of a flexible implicit prior boosts the learning of latent codes. However, the reviewer suggests that the contribution of the prior encoder is limited and that more ablation studies are needed to present the insights clearly. How do you respond to this evaluation?

Dialogue Agent: We appreciate the reviewer's positive feedback on the clarity of our pipeline and the importance of the implicit prior. We believe that our contribution in the prior encoder is significant due to our usage of the non-linear transformation and the structural consistency term. However, we understand that more ablation studies would help to clarify the role of each component in our proposed method. We will certainly consider this suggestion in the revised version of the paper.

Meta-Reviewer: Thank you for your response. Based on our discussion and my own evaluation of the reviews, I believe that the paper has potential and makes a significant contribution to the literature on representation learning with VAEs. However, I do agree with the reviewers that more work needs to be done to clarify the motivation, compare with more recent methods, and perform more ablation studies. Therefore, I recommend that the paper be given a conditional accept with the requirement that the authors revise the paper to address the concerns raised by the reviewers and provide more evidence to support their hypotheses.  Do you have any questions or concerns about this recommendation?

Dialogue Agent: No, we appreciate the feedback and will work hard to address the concerns raised by the reviewers and improve the paper to meet the expectations of the community. Thank you for your time and consideration.