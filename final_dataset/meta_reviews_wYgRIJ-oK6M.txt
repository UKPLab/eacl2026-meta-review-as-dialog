Meta-reviewer: Hello, dialogue agent. Can you tell me your overall assessment of the paper "BiT: Robustly Binarized Multi-distilled Transformer"?

Dialogue Agent: Based on the reviews provided, I would rate this paper as a solid piece of work with a high impact on the field. The authors propose a novel binarization technique for transformer models based on knowledge distillation that achieves competitive performance compared to a standard BERT on the GLUE benchmark and question answering. The paper also provides thorough ablation analysis, resources, and replicability measures, making it a technologically sound paper with no unaddressed ethical considerations.

Meta-reviewer: That sounds promising. Can you briefly describe the contributions of the paper?

Dialogue Agent: Sure. The authors propose a binarisation technique to improve knowledge distillation in transformer models, as well as a multi-step technique to distil binarised models, which improves intermediate performance. They also provide a clear description of the proposed approach, background knowledge and related work, and best practices for model replicability to enable reproducible research.

Meta-reviewer: Were there any weaknesses mentioned in the reviews?

Dialogue Agent: Two of the reviews mention concerns about the relation to other recent works and the potential impact on required hardware. While these concerns are valid, they do not affect the overall assessment of the paper as technically sound and impactful.

Meta-reviewer: I understand. Based on the reviews you provided, I'm inclined to accept this paper. However, Review 2 raises questions about the relationship with other recent works and the potential hardware required. Can you please provide more information on these concerns?

Dialogue Agent: Review 2 suggests that the authors should discuss the relationship with other recent works and compare the required hardware with traditional binary and ternary neural networks. Regarding the relationship with other recent works, the paper cites a large number of relevant works but does not discuss the similarity of the proposals in this work and related work in detail. As for the required hardware, the computational engine for BiT models needs to handle computation between 1 and -1 and 0 and 1. Review 2 suggests that the authors should compare this hardware with traditional binary and ternary neural networks for better justification.

Meta-reviewer: That is helpful additional information. Given these concerns, I'm still leaning towards accepting the paper. However, I also noticed that Review 1 expresses moderate originality. Does this have any implications for our decision?

Dialogue Agent: While the motivation for extreme quantization of neural networks is not new, this paper provides a valuable baseline for future work and increases the motivation for building special-purpose hardware for extremely low-precision neural networks. Therefore, I do not believe that the moderate originality expressed in Review 1 affects our decision to accept the paper.

Meta-reviewer: Thank you for your insights. Based on the reviews provided and our discussion, I have decided to accept the paper "BiT: Robustly Binarized Multi-distilled Transformer".