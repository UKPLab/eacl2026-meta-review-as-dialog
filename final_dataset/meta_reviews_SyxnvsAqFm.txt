 Sure, here is a multi-turn dialogue between a meta-reviewer and a dialogue agent for reviews about a paper titled "Computation-Efficient Quantization Method for Deep Neural Networks":

Turn 1:

Meta-Reviewer: Hello, I'm evaluating a paper titled "Computation-Efficient Quantization Method for Deep Neural Networks". Can you provide me with a summary of the paper and the reviews?

Dialogue Agent: Certainly! The paper proposes a method for efficient quantization of deep neural networks, which consists of two phases: a full-precision training phase and a quantized training phase. The reviews are [Review 1: This work addresses the issue of quantization for neural network, and in particular focus on Ternary weight networks. The proposed approach has two phases, the first phase performs quantization and de-quantization at certain iterations during training, where the schedule of these operations are hyperparameters specified a priori. The second phase focuses on training the scaling factor. The first phase is similar to the iterative quantization method proposed in Retraining-Based Iterative Weight Quantization for Deep Neural Networks, and differs in that this work performs the quantization and de-quantization operations more frequently.This work also proposed a modified version of L2 regularization, but its not clear how much benefit it provides compared to a regular L2 regularization. There is also a shuffling approach, but seems to provide limited improvement.The experiment results in general does not provide convincing evidence that the proposed method outperforms existing approaches. For example, the ResNet-32 on CIFAR-10 result does not perform better than the one reported in Trained ternary quantization, and the ImageNet result is also worse than some existing works.The work is lack of novelty and the results do not show significant improvement over existing approaches. |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct], [Review 2: The paper is a little hard to follow and some parts are poorly written. While the authors claim that they use the greedy approach (in sec 2) for quantization where both B and \alpha are learned in a greedy way, it is not clear why there is convergence difference between the two as claimed by the authors in section 3.1. Moreover, the authors claimed faster convergence of B than \alpha because fewer bit clips are observed from the left subplot of Figure 2. However, this conclusion is not quite convincing because 1) on the right subplot of Figure 2, it seems that \alpha also becomes more stable after 80k iterations; 2) the fewer bit clips may comes from using a stepwise learning rate decay scheme. Thus, the motivation for using another phase to train the \alpha is not strong.The iterative quantization approach has limited novelty. It is similar to many quantization methods like BinaryConnect and Xnor-Net, except that the quantization step is not done immediately after the BP and model updates, but after some iterations of full-precision training. Moreover, these methods also use full-precision weights for update during training.Clarity in the experiments section is a little better than the previous sections. However,- The proposed method only performs comparably with TTQ, and shows significant accuracy drop on the Cifar-10 and Cifar-100 datasets (especially on Cifar-100)- On the ImageNet dataset, there is a large accuracy drop of the proposed method compared to Zhou et al. (2018). Though the authors said that they believe their proposed model can reach a higher accuracy by using layer-by-layer quantization as in Zhou et al. (2018), it is hard to verify this claim due to lack of the corresponding results. Thus, efficacy of the proposed method on large datasets or models are hard to evaluate. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct], and [Review 3: This paper proposes a method based on re-training the full-precision model and then optimizing the corresponding binary model. It consists of two phases: (1) the full-precision model training where the quantization step is introduced through QSS to train the network, and (2) fine tuning of quantized networks, where the trained network was converted into a binary model. In addition, using the skewed matrix for quantization improves the accuracy. Then a loss function based on the k means form is used to normalize the weight for reducing the quantization error. Quantization experiments for CNNs or LSTMs have been conducted on CIFAR10, CIFAR100, IMAGENET, and WikiText-2 dataset. This paper has been presented clearly. However, it can be improved by introducing the motivation of the tricks(e.g. skewed matrix and loss related to k-means ) used for quantization.In the experiments, the precision improvement on the CIFAR and ImageNet dataset performs worse than some competitors. For example, the precision of the proposed method was significantly worse than Zhou et al, 2018 on ImageNet. It is better to analyze the reason. In addition, as claimed from the introduction, the contribution of this paper was to reduce the overhead of expensive quantization. However, no experimental results on computation time and parameter size have been shown. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct].

Turn 2:

Meta-Reviewer: Can you summarize the main points of the reviews for me?

Dialogue Agent: Sure! The reviews mention that the proposed method has limited novelty and does not provide convincing evidence of outperformance compared to existing approaches. The experiment results are also considered to be not sufficient to support the claims made in the paper. Additionally, the reviews suggest that the method is lacking in motivation and the results are not significant enough to justify the proposed approach.

Turn 3:

Meta-Reviewer: What are the strengths and weaknesses of the proposed method according to the reviews?

Dialogue Agent: According to the reviews, the proposed method has limited novelty and does not provide convincing evidence of outperformance compared to existing approaches. The experiment results are also considered to be not sufficient to support the claims made in the paper. Additionally, the reviews suggest that the method is lacking in motivation and the results are not significant enough to justify the proposed approach.

Turn 4:

Meta-Reviewer: How does the proposed method compare to existing approaches in terms of computational efficiency and accuracy?

Dialogue Agent: According to the reviews, the proposed method has limited novelty and does not provide convincing evidence of outperformance compared to existing approaches. The experiment results are also considered to be not sufficient to support the claims made in the paper. Additionally, the reviews suggest that the method is lacking in motivation and the results are not significant enough to justify the proposed approach. In terms of computational efficiency and accuracy, the proposed method is not significantly better than existing approaches.

Turn 5:

Meta-Reviewer: Based on the reviews, what is your final decision regarding the acceptance of the paper?

Dialogue Agent: Based on the reviews, I would recommend rejecting the paper due to lack of novelty, insufficient evidence of outperformance compared to existing approaches, and lack of motivation. The experiment results are also not sufficient to support the claims made in the paper.