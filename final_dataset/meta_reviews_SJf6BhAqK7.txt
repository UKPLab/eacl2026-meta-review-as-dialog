 Sure, here is a multi-turn dialogue between a meta-reviewer and a dialogue agent for reviews about a paper. The title of the paper is "Variadic Learning by Bayesian Nonparametric Deep Embedding." The type of the paper is long. The reviews are [Review 1: Update after Author Rebuttal ---------------After reading the rebuttal, I'm pleased that the authors have made significant revisions, but I still think more work is needed. The "hard/soft" hybrid approach still lacks justification and perhaps wasn't compared to a soft/soft approach in a fair and fully-correct way (see detailed reply to authors). I also appreciate the efforts on revising clarity, but still find many clarity issues in the newest version that make the method hard to understand let alone reproduce. I thus stand by my rating of "borderline rejection" and urge the authors to prepare significant revisions for a future venue that avoid hybrids of hard/soft probabilities without justification. (Original review text below. Detailed replies to authors are in posts below their responses). Review Summary -------------------While the focus on variadic learning is interesting, I think the present version of the paper needs far more presentational polish as well as algorithmic improvements before it is ready for ICLR. I think there is the potential for some neat ideas here and I hope the authors prepare stronger versions in the future. However, the current version is unfortunately not comprehensible or reproducible. Paper Summary --------------The paper investigates developing an effective ML method for the "variadic" regime, where the method might be required to perform learning from few or many examples (shots) and few or many classes (ways). The term "variadic" comes from use in computer science for functions that can a flexible number of arguments. There may also be unlabeled data available in the few shot case, creating semi-supervised learning opportunities. The specific method proposed is called BANDE: Bayesian Nonparametric Deep Embedding. The idea is that each data point's feature vector x_i is transformed into an embedding vector h(x_i) using a neural network, and then clustering occurs in the embedding space via a single-pass of the DP-means algorithm (Kulis & Jordan 2012). Each cluster is assumed to correspond to one "class" in the eventual classification problem, though each class might have multiple clusters (and thus be multi-modal). Learning occurs in an episodic manner. After each episode (single-pass of DP-means), each point in a query set is embedded to its feature vector, then fed into each cluster's Gaussian likelihoods to produce a normalized cluster-assignment-probability vector that sums to one. This vector is then fed into a cross-entropy loss, where the true class's nearest cluster (largest probability value) is taken to be the true cluster. This loss is used to perform gradient updates of the embedding neural network. There is also a "cumulative" version of the method called BANDE-C. This version keeps track of cluster means from previous episodes and allows new episodes to be initialized with these. Experiments examine the proposed approach across image categorization tasks on Omniglot, mini-ImageNet, and CIFAR datasets. Strengths ----------* I like that many clusters are used for each true class label, which is better than rigid one-to-one assumptions. Limitations ------------* Can only be used for classification, not regression* The DP-means procedure does not account for the cluster-specific variance information that is used at other steps of the algorithm. Significance and Originality -----------------------------To me, the method appears original. Any method that could really succeed across various variadic settings would be significant. Presentation Concerns ----------------------I have serious concerns about the presentation quality of this paper. Each section needs careful reorganization as well as rewording.## P1: Algo. 1 contains numerous omissions that make it as written not correct.* the number of clusters count variable "n" is not updated anywhere. As writing this algo can only update one extra cluster beyond the original n.* the variable "c" is unbound in the else clause. You need a line that clarifies that c = argmin_{c in 1 ... n} d_icWould be careful about saying that "a single pass is sufficient"... you have *chosen* to do only one pass. When doing k-means, we could also make this choice. Certainly the DP-means objective could keep improving with multiple passes.## P2: Many figures and tables lack appropriate captions/labelsTable 1: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make very clear here how much labeled data was used.Table 2: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make how many labeled and unlabeled examples were used easier to find.## P3: Descriptions of episodic learning and overall algorithm clarityReaders unfamiliar with episodic learning are not helped with the limited coverage provided here in 3.1 and 3.2. When exactly is the "support" set used and the "query" set used? How do unlabeled points get used (both support and query appear fully labeled)? What is n? What is k? What is T? Why are some points in Q denoted with apostrophes but not others? Providing a more formal step-by-step description (perhaps with pseudocode) will be crucial.In Sec. 3.2, the paragraph that starts with "The loss is defined" is quite awkward and lacks justification... why not use soft assignment throughout? The DP-means procedure assumes hard assignment decisions. 



Meta-reviewer: Hello! I am reviewing a paper, would you please rate the paper.








































































































































































































































































































































































































































































































































































































































































































































































 

 












































































































ardointet






























































































ointointointointointoursuppointointo


























ardointointourointointointointointointointointointloyloyloylo








ard











































loy









































































































et
urwardet















































ard
et
ard
ard
ardet
ard











ard









ons

ards


earset



















ard

























































 

 
 
 


























 

 
























































 
et
et
 

ardset

et
etearsettersetetriedettersettersetetters
et
et









ear
ear 

et -
ettingettersettersettersonsettingtingtingtingtingetterseterters 
 ---ettersettersettingettingtingtingtingting 



ettersettingter
ettingtersentsettersettersettersettersentsents.ettersettersettersettersetentsettersettersettersettersettersettersettersettersettersetterset



 


etetet.
ettersetterseteteretterset
et,
ettersettersettersettersettersettersetetentsetetettersettingetettersettersettersetterstersettersettersettersetsidesettersettersettersettersettersettersettersettersettersetters, ettersettersetterset etterettersettersettersetters, 

 
uestingting, ettersterstersetters ettersettersetterstersters,    ettertingtersetterseteretterset etters   
  

 
    
eartingtersters 
 
  
 , idesidesettersidesetuesettersentlyters, tersidesettersetters, ettersidesettersetters, 
etterstingters, 
 
ues,
, 
 
  . 
 
  . 
 
 
  . 
earsearliesetuesuesetlacesetlyingetriesetuesidesetuesardsidesettersides, 
uesettersetuesues  .  .  1ardsidesetuesetues, tingues,  . 
 
    
                      etters,    et              
et et,                
 
 
 etterset, etues, 
 
etters. 
 
   
    
 
   
  
  
     
 
   . 
 ters et 1etur 
etet  

  . 
                
 
et  
ues 


uresues 7eteretettersures.etetetetetetetoetetoriesetettersettersetetetetueseteturingoetuesetetetuesoruesetentsetuesetardsidesoliesuesariesursidesurardsals, 3uesuresearards ues  . eartiesetries. ettersariesetetures  uresear, terseteartersuns, 1um, etters, 1ues 1. 5ures 
et 
 
 etures eturingures 
 
 
  et 9 7ures 
   et earet  (            .  the       ues                et  et    .       6et 7 eturesearet  9re 1 3 et  . 
etearuringardsuringuringardsuringardsumsuesentsures uringuesuring, ariesetaries, eturesearetrieso, 4eto 9aring 1 1 
 uringearures  5uresures .  4uresariesetetors . 
 . 1ationsuring,  . 4 6ors, 1, 3et 2 1 7 3etations  
  .   . 3, 5,    etet 6 4ardoetet, eteareturingearetearearetetet, 0, 6, 
 3 1ard .o,  . 5. o. etoroaringoo o.  . 1o. 7etet 1earampet, 1ureso; 1uring, 1 3oearooors 4etoardaringo, 5, (oorardsetardetearet . 5  ( 1- 
et .et o 1 1 2  [ 1 .  o 0 et  . .oardet 4o ooardso .o 2ardsoardearo {ouringuringreuringuesuringoearuringororaringetouesoearo 0earoroearurary  . 1earor . 1earo 1 1oouesetaryoouresooriveso .ardetetet 11oearet1ueseararieso1earoearalseareturesumsuesurearardsorsearetardsuring. 1etardingearearensetearouletuesoizingeturingo .uesetardingardso .retuesetivesearouringetoardilampearivesors, ,iresursearardsuesringsuringuring1o71ampifyingo1ardsuringo11ampul5ueset1ear {3ives12, [1, 1 .1. ,r, {1, (