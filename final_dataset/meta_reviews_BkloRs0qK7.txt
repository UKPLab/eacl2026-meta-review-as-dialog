Meta-reviewer: Hi there, Dialogue Agent. I have read the reviews for the paper 'A comprehensive, application-oriented study of catastrophic forgetting in DNNs'. Overall, I'm leaning towards rejection due to the issues raised in Review 1 about unclear presentation of concepts and better linking of findings. However, Review 2 mentions that the paper could be valuable for the community and the issue it addresses is important. Could you provide me with more information about the paper type and the reviews?

Dialogue Agent: Certainly. The paper type is long, and it presents an empirical study of catastrophic forgetting in deep neural networks. It evaluates eight models against nine datasets with varying numbers of samples and constructs sequential learning tasks to test the models' performances in different scenarios. Review 1 rates the paper as marginally below acceptance threshold, and the reviewer is confident that the evaluation is correct but not absolutely certain. Review 2 rates the paper as marginally above acceptance threshold, and the reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature. 

Meta-reviewer: I see. Could you elaborate on the issues raised in Review 1 about the paper's unclear presentation of concepts and better linking of findings?

Dialogue Agent: According to Review 1, the comments made by the authors about presenting a novel training and model selection paradigm for incremental learning in DNNs are not justified. The reviewer suggests that better description of the results, such as in Table 3, should be presented, as well as better linking with the findings and a better structure of the latter to improve consistency. Review 1 suggests that these improvements could make the paper a candidate for a poster presentation.

Meta-reviewer: I find Review 1's points about unclear presentation to be concerning, but I agree with Review 2 that the issue of catastrophic forgetting in DNNs is important. What do you think about Review 3's suggestions for improving the paper's quality?

Dialogue Agent: Review 3 suggests that the analysis of the experiments is somewhat superficial and that including metrics proposed by Kemker et al. (2018) would improve the quality of the paper. Additionally, Review 3 suggests that section 2.4 describing the experimental setup could be clearer, and that 3D plots in section 4 could be replaced with 2D plots in the supplementary. Review 3 also suggests that the paper could cite and discuss Kemker et al. (2018), which conducted a somewhat similar experiment with fewer datasets but a larger number of classes. However, Review 3 rates the paper as good and recommends acceptance.

Meta-reviewer: I appreciate the suggestions made in Review 3 and agree that they could improve the quality of the paper. However, given the concerns expressed in Review 1 and the fact that Review 2 rates the paper only marginally above acceptance threshold, I think it would be best to reject the paper. Thank you for your responses.